+++
title = "Node Embeddings"
author = ["Victor Vialard"]
date = 2021-02-17
lastmod = 2021-03-02
draft = false
+++

tags
: [Network Science]({{<relref "20210214115728-network_science.md#" >}}) [Machine Learning]({{<relref "20201209095843-ml_org.md#" >}})

sources
: [Stanford CS224W](http://web.stanford.edu/class/cs224w)


## Why ? {#why}

-   Produce **task-independent features** for ML with graphs
    -   Avoid feature engineering
    -   Preserve the similarities in the graph (as vector product)
    -   Node2Vec : use **random walks** to optimize embeddings


## How ? {#how}


### Random Walk {#random-walk}

-   Setting
    -   \\(P(v | z\_{u})\\) : (predicted) probability of visiting node v from random walk starting at u
    -   \\(z\_{u}^{T} z\_{v}\\) : probability of co-occurrence in the graph

-   How to optimize ?
    -   _Expressiveness:_ local & higher order neighbour info
    -   _Efficiency:_ do not consider all pairs, but only those that co occur
    -   ⇒ \\(L =\sum\_{u \in V} \sum\_{v \in N\_{R}(u)} -\log (P(v|z\_{u}))\\), such that \\(P(v|z\_{u}) = \frac{\exp(z\_{u}^{T}z\_{v})}{\sum \exp(z\_{u}^{T}z\_{n})}\\)

-   Random walk algorithm
    -   Run fixed-random walks from each nodes
    -   Store \\(N\_{r}(u)\\) : multi-set (i.e. with repetitions) of nodes visited on random walks starting from u
    -   _Issue:_ expensive normalization term \\(\sum \exp(z\_{u}^{T}z\_{n}\\)


### DeepWalk ([Perozzi, Al-Rfou, and Skiena 2014](#org1f113b4)) {#deepwalk--perozzi-al-rfou-and-skiena-2014--org1f113b4}

-   Negative sampling
    -   Remove the normalization term
    -   Valid (form of Noise Contrastive Estimation)
    -   \\(\log \frac{\exp(z\_{u}^{T}z\_{v})}{\sum \exp(z\_{u}^{T}z\_{n})} \approx \log ( \sigma ( z\_{u}^{T} z\_{v} ) ) - \sum\_{i} log( \sigma ( z\_{u}^{T} z\_{n\_{i}} ) )\\), with k ~ 5 to 20

-   **DeepWalk:** Stochastic descent optimization, with fixed-length & unbiased random walks
    1.  Initialize \\(z\_{i}, \forall i\\) at random
    2.  Until convergence
        1.  Sample a node, and compute the derivative \\(\frac{\partial L^{(i)}}{\partial z\_{j}}\\)
        2.  Update, for all j : \\(z\_{j} \leftarrow z\_{j} - \eta \frac{\partial L^{(i)}}{\partial z\_{j}}\\)


### Node2Vec ([Grover and Leskovec 2016](#orgae98e03)) {#node2vec--grover-and-leskovec-2016--orgae98e03}

-   **Node2Vec:** _biased_ fixed-length random walks
    -   trade-off between local (BFS) vs global (DFS) to generate a node's neighbourhood
    -   At each step, three possibilities
        1.  \\(p\\) : return to previous node
        2.  \\(1\\) : stay same with
        3.  \\(q\\) (ratio DFS / DFS) : increase depth

-   Algorithm (all steps can be parallelised)
    1.  Compute random walk probabilities
    2.  Simulate random walks of length starting from each node \\(u\_{i}\\)
    3.  Optimize the node2vec objective using Stochastic Gradient Descent


## Bibliography {#bibliography}

<a id="orgae98e03"></a>Grover, Aditya, and Jure Leskovec. 2016. “Node2vec: Scalable Feature Learning for Networks.” _arXiv:1607.00653 [Cs, Stat]_, July.

<a id="org1f113b4"></a>Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. 2014. “DeepWalk: Online Learning of Social Representations.” _Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, August, 701–10. <https://doi.org/10.1145/2623330.2623732>.
