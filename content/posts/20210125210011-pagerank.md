+++
title = "PageRank"
author = ["Victor Vialard"]
lastmod = 2021-02-17
draft = false
+++

tags
: [Data Mining]({{< relref "20210125193220-data_mining" >}}) [Information Retrieval]({{< relref "20210124000000-information_retrieval" >}})

source
: [Stanford CS224W](http://web.stanford.edu/class/cs224w/) ([Page et al. 1999](#org6c9df0c))

## What is PageRank ? {#what-is-pagerank}

- **PageRank** algorithm

  - Count the number and quality of links to determine a web page's importance
  - More important pages are likely to receive more links
  - Solves an _eigenvalue_ problem using _random-walks_

- Graph analysis from a matrix perspective
  - Web as a _Directed Graph_ : nodes (webpages) & edges (hyperlink)
  - Note that the web evolved from a navigational setup (early web) to a transactional one

## The algorithm {#the-algorithm}

### Flow model {#flow-model}

- _Idea:_ links as votes

  - Each vote is proportional to a page's importance
  - Rank as \\(r\_{j} = \sum\_{i \rightarrow j}\frac{r\_{i}}{d\_{i}}\\), where :
    - \\(r\_{i}\\) is a page's importance
    - \\(d\_{i}\\) the number of out-links

- Matrix formulation : **stochastic adjacency matrix**
  - \\(M\_{ij} = \frac{1}{d\_{j}}\\) if \\(j \rightarrow i\\), which leads to \\(\sum\_{i}M\_{ij} = 1\\)
  - Rank vector r : importance of each page \\(\sum r\_{i} = 1\\)
  - **Flow equation** : \\(r = M \cdot r\\)

### Random Walk {#random-walk}

- Connection to a **random walk**
  - \\(p(t)\\) : probability distribution over pages
  - \\(p(t+1) = M \cdot p(t)\\) : the surfer follows a link uniformly at random
  - \\(p(t+1) = p(t)\\) : surfer's stationary distribution
  - So page rank computes \\(r\\), which is the **stationary distribution** of a random walk over a graph

### Power Iteration {#power-iteration}

- PageRank uses **power iteration** to compute the ranks
  - Steps
    1.  Init : \\(r⁰ = [1/N]^{T}\\)
    2.  Iter : \\(r^{t+1} = M \cdot r^{t}\\)
    3.  Stop : \\(| r^{(t+1)} - r^{t} | < \epsilon\\)
  - 50 iterations are sufficient to estimate the limiting solution

### Pitfalls & final solution {#pitfalls-and-final-solution}

- Two problems :

  1.  _Spider traps_
      - Confine the crawler in a set of pages
      - The surfer can _teleport_ to a random page with a probability \\(1-\beta\\), where \\(\beta \approx 0.8\\)
  2.  _Dead ends_
      - Randomly teleport when reaching a dead end

- But how to compute the correct ranks ?
  - Single stochastic equation
  - **PageRank algorithm**
    - \\(r\_{j} = \sum\_{i \rightarrow j} \beta \frac{r\_{i}}{d\_{i}} + (1 - \beta) \frac{1}{N}\\)

### Extensions {#extensions}

- PPR : Personalized PageRank
  - Randomly teleport to the pages the user visited

## Bibliography {#bibliography}

<a id="org6c9df0c"></a>Page, Lawrence, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. “The PageRank Citation Ranking: Bringing Order to the Web.” Stanford InfoLab.
